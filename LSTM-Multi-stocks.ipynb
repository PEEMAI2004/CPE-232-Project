{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21329ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 14:48:28.120900: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-24 14:48:28.124165: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-24 14:48:28.132361: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745480908.146628   52152 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745480908.150609   52152 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745480908.162704   52152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745480908.162736   52152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745480908.162738   52152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745480908.162739   52152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-24 14:48:28.167023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8adc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META', 'TSLA', 'NFLX', 'NVDA', 'INTC', 'AMD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cb551",
   "metadata": {},
   "source": [
    "### Download 2 Years of Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b25cde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = yf.download(ticker, period=\"2y\")\n",
    "    df.dropna(inplace=True)\n",
    "    data[ticker] = df[['Open', 'High', 'Low', 'Close', 'Volume']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d981a39",
   "metadata": {},
   "source": [
    "Save Data to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7785b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data in df to CSV\n",
    "for ticker, df in data.items():\n",
    "    df.to_csv(f'Stocks/{ticker}_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9037e5",
   "metadata": {},
   "source": [
    "Load Data from .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58166ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
      "/tmp/ipykernel_52152/4098107145.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from CSV\n",
    "data = {}\n",
    "for ticker in tickers:\n",
    "    df = pd.read_csv(f'Stocks/{ticker}_data.csv', index_col=0, parse_dates=True)\n",
    "    # Convert columns to numeric, coercing errors to NaN, and drop rows with NaN values\n",
    "    df[['Open', 'High', 'Low', 'Close']] = df[['Open', 'High', 'Low', 'Close']].apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "    data[ticker] = df[['Open', 'High', 'Low', 'Close', 'Volume']].astype({'Open': 'float64', 'High': 'float64', 'Low': 'float64', 'Close': 'float64', 'Volume': 'int64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf1e29",
   "metadata": {},
   "source": [
    "### Build Dataset (Sequences + Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cdc53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 60\n",
    "PREDICT_DAYS = 30\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for ticker, df in data.items():\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    for i in range(len(scaled) - WINDOW - PREDICT_DAYS):\n",
    "        X_seq = scaled[i:i+WINDOW]\n",
    "        close_start = df.iloc[i+WINDOW-1]['Close']\n",
    "        close_end = df.iloc[i+WINDOW+PREDICT_DAYS-1]['Close']\n",
    "        growth = (close_end - close_start) / close_start\n",
    "        X.append(X_seq)\n",
    "        y.append(growth)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909379d9",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed46c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1fd9d",
   "metadata": {},
   "source": [
    "### Build the LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae5b81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 14:51:15.732801: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/kamin/Documents/CPE 232 Project/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m17,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,985</span> (70.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,985\u001b[0m (70.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,985</span> (70.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,985\u001b[0m (70.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(X.shape[1], X.shape[2]), return_sequences=False),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ff5d0",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de0f9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0189 - val_loss: 0.0194\n",
      "Epoch 2/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0188 - val_loss: 0.0187\n",
      "Epoch 3/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0179 - val_loss: 0.0191\n",
      "Epoch 4/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0182 - val_loss: 0.0186\n",
      "Epoch 5/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0177 - val_loss: 0.0188\n",
      "Epoch 6/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0167 - val_loss: 0.0192\n",
      "Epoch 7/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0173 - val_loss: 0.0184\n",
      "Epoch 8/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0180 - val_loss: 0.0184\n",
      "Epoch 9/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0174 - val_loss: 0.0183\n",
      "Epoch 10/10\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0181 - val_loss: 0.0184\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10328f1",
   "metadata": {},
   "source": [
    "### Predict Growth on Latest Data & Pick Top Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d97f160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 30-day growth:\n",
      "AAPL: 0.0276\n",
      "GOOGL: 0.0488\n",
      "MSFT: 0.0546\n",
      "AMZN: 0.0827\n",
      "META: 0.0929\n",
      "TSLA: -0.0443\n",
      "NFLX: 0.0273\n",
      "NVDA: 0.0388\n",
      "INTC: 0.0539\n",
      "AMD: 0.0360\n",
      "\n",
      "📈 Best stock to pick: META\n"
     ]
    }
   ],
   "source": [
    "def predict_next_growth(df, window=60):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(df[-window:])\n",
    "    X_input = np.expand_dims(scaled, axis=0)\n",
    "    pred = model.predict(X_input, verbose=0)\n",
    "    return pred[0][0]\n",
    "\n",
    "predictions = {ticker: predict_next_growth(df) for ticker, df in data.items()}\n",
    "top_stock = max(predictions, key=predictions.get)\n",
    "\n",
    "print(\"Predicted 30-day growth:\")\n",
    "for t, p in predictions.items():\n",
    "    print(f\"{t}: {p:.4f}\")\n",
    "\n",
    "print(f\"\\n📈 Best stock to pick: {top_stock}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e68a5a",
   "metadata": {},
   "source": [
    "### Prepare Data for All 10 Stocks in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "afae3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 60\n",
    "PREDICT_DAYS = 30\n",
    "NUM_STOCKS = len(tickers)\n",
    "\n",
    "# Ensure all stocks have the same length\n",
    "min_len = min(len(df) for df in data.values())\n",
    "for k in data:\n",
    "    data[k] = data[k].iloc[-min_len:]\n",
    "\n",
    "# Stack all stocks into a 3D array: (samples, 10, 60, 5)\n",
    "X_multi, y_multi = [], []\n",
    "\n",
    "for i in range(min_len - WINDOW - PREDICT_DAYS):\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    for ticker in tickers:\n",
    "        df = data[ticker]\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled = scaler.fit_transform(df.iloc[i:i+WINDOW])\n",
    "        X_seq.append(scaled)\n",
    "\n",
    "        # Label: growth over next 30 days\n",
    "        close_start = df.iloc[i+WINDOW-1]['Close']\n",
    "        close_end = df.iloc[i+WINDOW+PREDICT_DAYS-1]['Close']\n",
    "        growth = (close_end - close_start) / close_start\n",
    "        y_seq.append(growth)\n",
    "\n",
    "    X_multi.append(X_seq)\n",
    "    y_multi.append(y_seq)\n",
    "\n",
    "X_multi = np.array(X_multi)  # shape: (samples, 10, 60, 5)\n",
    "y_multi = np.array(y_multi)  # shape: (samples, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac5ecb5",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0104cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_multi, y_multi, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddafcae",
   "metadata": {},
   "source": [
    "###  Build the Multi-stock, Multi-output LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "daa7c830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,920</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m5\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m17,920\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │            \u001b[38;5;34m65\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,985</span> (70.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,985\u001b[0m (70.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,985</span> (70.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,985\u001b[0m (70.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed, LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(NUM_STOCKS, WINDOW, 5))  # (batch, 10, 60, 5)\n",
    "\n",
    "# LSTM per stock via TimeDistributed\n",
    "x = TimeDistributed(LSTM(64))(input_layer)  # (batch, 10, 64)\n",
    "output_layer = Dense(1)(x)  # (batch, 10, 1)\n",
    "output_layer = tf.keras.layers.Lambda(lambda t: tf.squeeze(t, axis=-1))(output_layer)  # (batch, 10)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c3042",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "950c679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 139ms/step - loss: 0.0268 - val_loss: 0.0198\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0205 - val_loss: 0.0186\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0210 - val_loss: 0.0181\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0200 - val_loss: 0.0180\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.0188 - val_loss: 0.0181\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.0203 - val_loss: 0.0181\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0199 - val_loss: 0.0182\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.0205 - val_loss: 0.0181\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0200 - val_loss: 0.0178\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.0191 - val_loss: 0.0180\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913aa47",
   "metadata": {},
   "source": [
    "### Predict Best Stock from Latest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e33f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 30-day growth per stock:\n",
      "AAPL: -0.0045\n",
      "GOOGL: -0.0141\n",
      "MSFT: -0.0126\n",
      "AMZN: -0.0045\n",
      "META: -0.0103\n",
      "TSLA: -0.0379\n",
      "NFLX: -0.0067\n",
      "NVDA: -0.0102\n",
      "INTC: 0.0125\n",
      "AMD: 0.0025\n",
      "\n",
      "📈 Best stock to pick: INTC\n"
     ]
    }
   ],
   "source": [
    "# Prepare last 60 days for each stock\n",
    "X_latest = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = data[ticker]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(df[-WINDOW:])\n",
    "    X_latest.append(scaled)\n",
    "\n",
    "X_latest = np.expand_dims(X_latest, axis=0)  # shape: (1, 10, 60, 5)\n",
    "\n",
    "# Predict growth for all 10 stocks\n",
    "growths = model.predict(X_latest, verbose=0)[0]\n",
    "growth_dict = dict(zip(tickers, growths))\n",
    "\n",
    "top_stock = max(growth_dict, key=growth_dict.get)\n",
    "\n",
    "print(\"Predicted 30-day growth per stock:\")\n",
    "for t, g in growth_dict.items():\n",
    "    print(f\"{t}: {g:.4f}\")\n",
    "\n",
    "print(f\"\\n📈 Best stock to pick: {top_stock}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f648db",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3271bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.018014\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step \n",
      "RMSE: 0.134217\n",
      "MAE: 0.097405\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "val_loss = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation MSE: {val_loss:.6f}\")\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((y_pred - y_val) ** 2))\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae = np.mean(np.abs(y_pred - y_val))\n",
    "print(f\"MAE: {mae:.6f}\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
